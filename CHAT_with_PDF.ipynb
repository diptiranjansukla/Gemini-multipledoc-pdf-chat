{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install important dependencies"
      ],
      "metadata": {
        "id": "iYplJyWItlPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-Z66ghD_OvB",
        "outputId": "7505b4af-20e7-4700-9c8a-5cbbf8c4461d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.8)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.21)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.24)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.24->langchain) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.24->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.22)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.109.2)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.17.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import important libraries"
      ],
      "metadata": {
        "id": "36i8-Et-t26N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "Hp3Y2gJH_Tln"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/day 1.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "4DzWz24h_jNV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkmavxzQ_p_4",
        "outputId": "9514264b-2c56-4a4a-84b6-72ffc43bfa14"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)', metadata={'source': '/content/day 1.pdf', 'page': 0}), Document(page_content='INEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.', metadata={'source': '/content/day 1.pdf', 'page': 1}), Document(page_content='INEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification', metadata={'source': '/content/day 1.pdf', 'page': 2}), Document(page_content=\"INEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) .\", metadata={'source': '/content/day 1.pdf', 'page': 3}), Document(page_content=\"INEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value .\", metadata={'source': '/content/day 1.pdf', 'page': 4}), Document(page_content=\"INEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.\", metadata={'source': '/content/day 1.pdf', 'page': 5}), Document(page_content='INEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization .', metadata={'source': '/content/day 1.pdf', 'page': 6}), Document(page_content='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.', metadata={'source': '/content/day 1.pdf', 'page': 7}), Document(page_content='INEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.', metadata={'source': '/content/day 1.pdf', 'page': 8}), Document(page_content='INEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror.', metadata={'source': '/content/day 1.pdf', 'page': 9}), Document(page_content='INEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line', metadata={'source': '/content/day 1.pdf', 'page': 10}), Document(page_content='INEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line', metadata={'source': '/content/day 1.pdf', 'page': 11})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c82gqeuHA-Ds",
        "outputId": "b2eadfd4-008a-47d3-f4aa-c9aec3abd98a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)', metadata={'source': '/content/day 1.pdf', 'page': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the Extracted Data into Text Chunks"
      ],
      "metadata": {
        "id": "f0W1PGUGtdQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n"
      ],
      "metadata": {
        "id": "X4jCCBhZBHgq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_text(context)"
      ],
      "metadata": {
        "id": "r8EbDpxMBmUb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36WOJBzbBrvP",
        "outputId": "ac50a320-0bcb-4efa-a9d7-361d88e0af60"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "aqPzQ9v8BuHx",
        "outputId": "c9cfa48d-d888-462e-9cb6-7ea91c796c4c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)\\n\\nINEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.\\n\\nINEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification\\n\\nINEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) .\\n\\nINEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value .\\n\\nINEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.\\n\\nINEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "IwKEi9AkCEHQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "fu3Tf3_5CRqA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n"
      ],
      "metadata": {
        "id": "snAW9UvxBxQI"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Provide your Google API key here: \")\n"
      ],
      "metadata": {
        "id": "C9RTftGCDCR5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "#     os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Provide your Google API key here\")"
      ],
      "metadata": {
        "id": "wdxO2W9ZCi58"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Embeddings from Google"
      ],
      "metadata": {
        "id": "7YSdaNgctVQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "tsDyykPmB0lW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSex6E_FCeIX",
        "outputId": "20fa06e9-6d21-44d1-9b6e-c891344502c6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.22)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.109.2)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.17.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.2)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (29.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.1.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "-mVBrAXxDRs1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings for each of the Text Chunk and save them into a Vector Store"
      ],
      "metadata": {
        "id": "5NOZsXbwtOoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever()"
      ],
      "metadata": {
        "id": "73ODoFP1DU9P"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the difference between AI,  Data  Science , ML, and DL ?\"\n",
        "docs = vector_index.get_relevant_documents(question)"
      ],
      "metadata": {
        "id": "fo5CBhrpDcTS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Bmbv_oD6Mm",
        "outputId": "c296fe3d-ad97-4411-f10d-bc073107ed5a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)\\n\\nINEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.\\n\\nINEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification\\n\\nINEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) .\\n\\nINEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value .\\n\\nINEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.\\n\\nINEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization .\"),\n",
              " Document(page_content=\"Data  Science  \\nInterview  Questions  \\n(30 days of Interview  Preparation)\\n\\nINEURON.AI  \\n \\n Page 2 \\n  \\nQ1.  What is the difference between AI,  Data  Science , ML, and DL ? \\n \\nAns 1 :  \\n         \\n  \\n \\nArtificial Intelligence : AI is purely math and scientific exercis e, but when it became computa tional , it \\nstarted to solve human problems formalized into a subset of computer science. A rtificial intelligence has \\nchanged the original computational statistics paradigm to the modern idea that machines could mimic \\nactual human capabilities, such as deci sion making and perfo rming more “human” tasks. Modern AI into \\ntwo categories  \\n1. General AI - Planning, decision making, identifying objects, recognizing sounds, social & \\nbusiness transactions  \\n2. Applied AI - driverless/ Autonomous car or machine smartly trade st ocks \\n \\nMachine Learning : Instead of engineers “teaching” or programming computers to have what they need \\nto carry out tasks, that perhaps computers could teach themselves – learn something without being \\nexplicitly programmed to do so. ML is a form of AI where  based on more data , and they can change \\nactions and response, which will make more effic ient, ad aptable and scalable. e .g., navigation apps and \\nrecommendation engine s. Classified into: - \\n1. Supervised  \\n2. Unsupervised  \\n3. Reinforcement learning  \\n \\nData Science : Data science ha s many tools, techniques, and algorithms called from these fields, plus \\nothers –to handle big data  \\nThe goal of data science, somewhat similar to machine learning, is to make accurate predictions and to \\nautomate and perform transactions in real -time, such as purchasing internet traffic or automatically \\ngenerating content.\\n\\nINEURON.AI  \\n \\n Page 3 \\n Data science relies less on math and coding and more on data and building new systems to process the \\ndata. Relying on the fields of data integration, distributed architecture, aut omated machine learning, data \\nvisualization, data engineering, and automated data -driven decisions, data science can cover an entire \\nspectrum of data processing, not only the algorithms or statistics related to data.  \\n \\nDeep Learning : It is a technique for i mplementing ML.  \\nML provides the desired output from a given input , but DL reads the input and applies it to another data.  \\nIn ML , we can easily classify the flower  based upon the features . Suppose you want a machine to look at \\nan image and determine what  it represents to the human eye , whether a face, flower, landscape, truck, \\nbuilding, etc.  \\nMachine learning is not sufficient for this task because machine learning can only produce an output from \\na data set – whether according to a known algorithm or based on the inherent structure of the data. Y ou \\nmight be able to use machine learning to determine whether an image was of an “X” – a flower, say – and \\nit would learn and get more accurate. But that output is binary (yes/no) and is dependent on the \\nalgorithm, not the data. In the image recognition ca se, the outcome is not binary and not dependent on \\nthe algorithm.  \\nThe n eural network performs MICRO calculations with computational on many layers . Neural networks \\nalso support weighting data for ‘confidence . These results in a probabilistic system , vs. deterministic, and \\ncan handle tasks that we think of as requiring more ‘human -like’ judg ment.  \\n \\n \\nQ2. What is the difference  between Supervised learning, U nsupervised  learning and \\nReinforcement  learning ? \\n \\nAns 2 :  \\nMachine Learning  \\nMachine learning is the scient ific study of algorithms and statistical models that computer systems use to \\neffectively perform a specific task without using explicit instructions, relying on patterns and inference \\ninstead.  \\nBuilding a model by learning the patterns of historical data wi th some relationship between data to make \\na data -driven prediction.  \\n \\nTypes of Machine Learning  \\n• Supervised Learning  \\n• Unsupervised Learning  \\n• Reinforcement Learning  \\n \\nSupervised learning  \\nIn a supervised learning model, the algorithm learns on a labe led da taset, to generate reasonable \\npredictions for the response to new data.  (Forecasting outcome of new data)  \\n• Regression  \\n• Classification\\n\\nINEURON.AI  \\n \\n Page 4 \\n  \\nUnsupervised learning  \\nAn unsupervised model, in contrast , provides unlabelled data that the algorithm tries to make sens e of by \\nextracting features, co -occur rence and underlying patterns on its own. We use unsupervised learning for  \\n• Clustering  \\n• Anomaly detection  \\n• Association  \\n• Autoencoders  \\nReinforcement Learning  \\nReinforcement learning is less supervised and depends on  the learning agent in determining the output \\nsolutions by arriving at different possible ways to achieve the best possible solution.  \\n \\nQ3. Describe the general architecture of Machine learning . \\n \\n          \\n  \\n \\n \\nBusiness understanding : Understand the give use cas e, and also , it's good to know more about the \\ndomain for which the use cases are buil t. \\n \\nData Acquisition and Understanding : Data gathering from different sources and understanding the \\ndata. Cleaning the data , handling the missing data if any , data wrangli ng, and EDA( Exp loratory data \\nanalysis) .\\n\\nINEURON.AI  \\n \\n Page 5 \\n Modeling:  Feature Engineering  - scaling the data , feature selection - not all feature s are important. We \\nuse the backward elimination method , correlation factors, PCA and domain knowledge to select the \\nfeatures.  \\nModel Training  based on trial and error method or by experience , we select the al gorithm and train with \\nthe selected features.  \\nModel evaluation  Accuracy of the model , confusion matrix and cross -validation.  \\nIf accuracy is not high , to achi eve higher accuracy , we tune  the model...either by changing the algo rithm \\nused or by feature selection or by g athering more data , etc. \\nDeployment  - Once the model has good accuracy , we deplo y the model either in the cloud or Rasberry \\npy or any other place. Once we deploy , we monitor the performa nce of the model.if its good...we go live \\nwith the model or re iterate the all process unti l our model performa nce is good.  \\nIt's not done yet!!!  \\nWhat if , after a few day s, our model performs bad ly because of new data . In that case , we do all the \\nprocess a gain by collecting new data and redeploy the model.  \\n \\nQ4. What is Linear Regression ? \\n \\nAns 4: \\nLinear Regression tends to establish a relationship between a depend ent variable(Y) and one or more \\nindepend ent variable(X) by finding the best fit of the straight line.  \\nThe equation for the Linear model is Y = mX+c, where m is the slope and c is the intercept  \\n \\n \\nIn the above diagram, the blue dots we see are the distribution of 'y' w.r.t 'x .' There is no straight line that \\nruns through all the data points. So, the objective here is to fit the best fit of a straight line that will try to \\nminimi ze the error between the expected and actual value .\\n\\nINEURON.AI  \\n \\n Page 6 \\n  \\nQ5. OLS Stats Model (Ordinary Least Square)  \\n \\nAns 5: \\nOLS is a stats model, which will help us in identifying the more significan t features that can has an \\ninfluence on the ou tput. OLS model in python is  executed  as: \\nlm = smf.ols(formula = 'Sales ~ am+constant', data = data).fit() lm.conf_int() lm.summary()  \\nAnd we get the output as below,  \\n \\n \\nThe higher the t -value for the feature, the more significant the feature is to the output variable . And \\nalso, the p -value plays a rule in rejecting the Null hypothesis(Null hypothesis stating the features has zero \\nsignificance on the target variable.).  If the p -value is less than 0.05(95% confidence interval) for a \\nfeature, then we  can consider the feature to be significant.  \\n \\n \\nQ6. What is L1 Regularization (L1 = lasso)  ? \\n \\nAns 6: \\nThe main objective of creating a model(training data) is mak ing sure it fits the data properly and reduce \\nthe loss. Sometimes the model that is trained which will fit the data b ut it may fail and give a poor \\nperformance during analyzing of data (test data) . This leads to overfitting. Regularization came to \\novercome overfitting.  \\nLasso Regression ( Least Absolute Shrinkage and Selection Operator ) adds “ Absolute value of \\nmagnit ude” of coefficient , as penalty term to the loss function.\\n\\nINEURON.AI  \\n \\n Page 7 \\n Lasso shrinks the less important feature’s coefficient to zero ; thus, removing some feature altogether. So, \\nthis works well for feature selection in case we have a huge number of features.  \\n \\n \\n \\nMethods like Cross-validation, Stepwise Regression are there  to handle overfitting  and perform feature \\nselection work well with a small set of features . These techniques are  good when we are dealing with a \\nlarge set of features.  \\nAlong with sh rinking coefficients,  the lasso performs feature selection , as well. (Remember the \\n‘selection‘ in the lasso full -form?) Because some of the coefficients become exactly zero, which is \\nequivalent to the particular feature being excluded from the model.  \\n \\nQ7. L2 Re gularization(L2 = Ridge Regression)  \\n \\nAns 7: \\n \\n \\nOverfitting happens when the model learns signal as well as noise in the training data and wouldn’t \\nperform well on new/unseen data on which model wasn’t trained on.  \\nTo avoid overfitting your model on training data like  cross -validation sampling , reducing the number \\nof features, pruning , regularization , etc. \\nSo to  avoid overfitting , we perform  Regularization .\"),\n",
              " Document(page_content='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.\\n\\nINEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.\\n\\nINEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror.\\n\\nINEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line\\n\\nINEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line'),\n",
              " Document(page_content='INEURON.AI  \\n \\n Page 8 \\n  \\n \\nThe Regression model that uses  L2 regularization is called Ridge Regression.  \\nThe f ormula for Ridge Regression :-  \\n \\n \\nRegula rization adds the penalty as model complexity increases. The r egularization parameter \\n(lambda) penalizes all the parameters except intercept so that the model generalizes the data and \\nwon’t overfit.  \\nRidge regression adds  “squared magnitude of the coefficient \" as penalty term to the loss \\nfunction. Here the box part in the above image represents the L2 regularization element/term.  \\n \\n \\nLambda is a hyperparameter.\\n\\nINEURON.AI  \\n \\n Page 9 \\n If lambda is zero , then it is equivalent to OLS. But if lambda is very large , then it will add too much \\nweight , and it will lead to under -fitting . \\nRidge regularization  forces the weights to be small but does not make them zero  and does no t give \\nthe sparse solution . \\nRidge is not robust to outliers  as square terms blow  up the error differences of the outlie rs, and the \\nregularization term tries to fix it by penalizing the weights  \\nRidge regression performs better when all the input features influence the output , and all with  weights \\nare of roughly equal size . \\nL2 regularization can learn complex data patte rns. \\n \\nQ8. What is R square(where to use and where not) ? \\nAns 8. \\nR-squared  is a statistical measure of how close the data are to the fitted regression line. It is also \\nknown as the  coefficient of determination, or the coefficient of multiple determination for multiple \\nregre ssion.  \\nThe definition of R -squared is the  percentage of the response variable variation that is explained by a \\nlinear model.  \\nR-squared = Explained variation / Total variation  \\nR-squared is always between 0 and 100%.  \\n0% indicates that the model explains none  of the variability of the response data around its mean.  \\n100% indicates that the model explains all the variability of the response data around its mean.  \\nIn general, the higher the R -squared, the better the model fits your data.\\n\\nINEURON.AI  \\n \\n Page 10 \\n There is a  problem w ith the R -Square. The problem arises when we ask this question to ourselves.** Is it \\ngood to help as many independent variables as possible?**  \\nThe answer is No  because we understood that each independent variable should have a meaningful \\nimpact. But, even**  if we add independent variables which are not meaningful**, will it improve R -Square \\nvalue?  \\nYes, this is the basic problem with R -Square. How many junk independent variables or important \\nindependent variable or impactful independent variable you add to y our model, the R -Squared value will \\nalways increase. It will never decrease with the addition of a newly independent variable , whether it could \\nbe an impactful, non -impactful , or bad variable, so we need another way to measure equivalent R -\\nSquare , which penalizes our model with any junk independent variable.   \\nSo, we calculate the  Adjusted R -Square  with a better adjustment in the formula of generic R -square.  \\n \\n \\nQ9. What is Mean Square  Error? \\nThe mean squared error tells you how close a regression line is to a set of points. It does this by \\ntaking the distances from the points to the regression line (these distances are the “errors”) and \\nsquaring them.  \\n \\nGiving an intuition  \\n \\n \\n \\nThe line equation is y=Mx+B . We want to find M (slope)  and B (y-intercept)  that minimizes the \\nsquared e rror.\\n\\nINEURON.AI  \\n \\n Page 11 \\n  \\n \\n \\n \\nQ10. Why S upport  Vector Regre ssion ? Difference between SVR and a simple regression \\nmodel?  \\n \\nAns 10: \\nIn simple linear regression , try to minimi ze the error rate. But in SVR , we try to  fit the er ror within \\na certain threshold . \\n \\nMain Concep ts:- \\n1. Boundary  \\n2. Kernel  \\n3. Support Vector  \\n4. Hyper Plane  \\n \\n \\n \\n \\nBlue line: Hyper Plane; Red Line: Boundary -Line\\n\\nINEURON.AI  \\n \\n Page 12 \\n Our best fit line is the one w here th e hyperplane  has the maximum number of points.  \\nWe are trying to do here is trying to decide a decision boundary at ‘ e’ distance from the original \\nhyper plane such that data points closest to the hyper plane or the support vectors are within that \\nboundary line')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Prompt Template"
      ],
      "metadata": {
        "id": "a4guuJLytDy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "  Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
        "  provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
        "  Context:\\n {context}?\\n\n",
        "  Question: \\n{question}\\n\n",
        "\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n"
      ],
      "metadata": {
        "id": "RYt6QKNYD-O-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.3)"
      ],
      "metadata": {
        "id": "R60u6nJWESyq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)"
      ],
      "metadata": {
        "id": "f4_AmTqbEU4L"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)"
      ],
      "metadata": {
        "id": "ZWhQ4MJwEZAB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83T7pWD9EciD",
        "outputId": "97ec88de-2c96-43da-dc3b-62f4e53b5a58"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': '**Artificial Intelligence (AI)** is a broad field that encompasses the study and development of intelligent agents, which are systems that can reason, learn, and act autonomously. AI has been used to create a wide range of applications, including self-driving cars, facial recognition software, and natural language processing systems.\\n\\n**Data Science** is a field that combines statistics, computer science, and domain knowledge to extract insights from data. Data scientists use a variety of techniques to analyze data, including machine learning, data mining, and visualization. Data science has been used to solve a wide range of problems, including fraud detection, customer segmentation, and product recommendation.\\n\\n**Machine Learning (ML)** is a subfield of AI that focuses on the development of algorithms that can learn from data. ML algorithms can be used to solve a variety of problems, including classification, regression, and clustering. ML has been used to create a wide range of applications, including spam filters, image recognition software, and predictive analytics systems.\\n\\n**Deep Learning (DL)** is a subfield of ML that uses artificial neural networks to learn from data. DL algorithms have been shown to be very effective at solving a wide range of problems, including image recognition, natural language processing, and speech recognition. DL has been used to create a wide range of applications, including self-driving cars, facial recognition software, and natural language processing systems.'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}